# batchnormalized-mlp
Building an fully-connected neural net but with a few additional hacks like:
    -BatchNormalization and Kaiming init applied at weights initialization:)
